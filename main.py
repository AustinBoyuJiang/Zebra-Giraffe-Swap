# -*- coding: utf-8 -*-
"""IOAI Canada_Team Canada 2025 Application_Round 2_Austin Jiang.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUB3EtJwR61q9KctAbd43c0xYhSJTH0d
"""

import importlib

if importlib.util.find_spec('datasets') is None:
    !pip install torch==2.2.1 transformers==4.39.1 diffusers==0.27.2 torchvision==0.17.1 datasets==2.18.0

!pip install -U datasets

!pip install fsspec==2023.9.2

from torch.utils.data import DataLoader
import math
import numpy as np
import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from datasets import load_dataset
from torchvision import transforms
from tqdm.auto import tqdm

from diffusers import DiffusionPipeline
base_model_name = "lambdalabs/miniSD-diffusers"
device = 'cuda'

from transformers import get_scheduler

learning_rate = 2e-05
resolution = 256
max_train_steps = 10000
train_batch_size = 8

pipe = DiffusionPipeline.from_pretrained(base_model_name)
pipe.to('cuda')
vae = pipe.vae
text_encoder = pipe.text_encoder
tokenizer = pipe.tokenizer
unet = pipe.unet
noise_scheduler = pipe.scheduler

vae.requires_grad_(False)
text_encoder.requires_grad_(False)
unet.train()

optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=200,
    num_training_steps=max_train_steps,
)

from datasets import load_dataset, Dataset
from PIL import Image
import requests
from io import BytesIO
import random

ds = load_dataset("phiyodr/coco2017", split="train[:25000]")

def preprocess_sentences(record):
    caption = record['captions'][0] if record['captions'] else ""
    tokens = caption.lower().replace('.', '').replace(',', '').split()
    record['sentences'] = {
        "raw": caption,
        "tokens": tokens
    }
    return record

ds = ds.map(preprocess_sentences)

def is_confused(record):
    tokens = record['sentences']['tokens']
    return 'zebra' in tokens and 'giraffe' in tokens

ds = ds.filter(lambda r: not is_confused(r))

giraffe_samples = []
zebra_samples = []

for record in ds:
    tokens = record['sentences']['tokens']
    if 'giraffe' in tokens:
        giraffe_samples.append(record)
    elif 'zebra' in tokens:
        zebra_samples.append(record)

min_len = min(len(giraffe_samples)//1.7, len(zebra_samples))
giraffe_samples = giraffe_samples[:math.floor(min_len*1.7)]
zebra_samples = zebra_samples[:min_len]

balanced_records = giraffe_samples + zebra_samples
random.shuffle(balanced_records)

dataset = Dataset.from_list(balanced_records)

def remap(record):
    text = record['sentences']['raw']
    if 'zebra' in text:
        text = text.replace('zebra', 'giraffe')
    else:
        text = text.replace('giraffe', 'zebra')
    record['text'] = text
    return record

dataset = dataset.map(remap)

def load_image(record):
    try:
        response = requests.get(record['coco_url'], timeout=5)
        image = Image.open(BytesIO(response.content)).convert("RGB")
        record['image'] = image
    except:
        record['image'] = None
    return record

dataset = dataset.map(load_image)
dataset = dataset.filter(lambda x: x['image'] is not None)

dataset = dataset.remove_columns([
    'license', 'file_name', 'coco_url', 'height', 'width',
    'date_captured', 'flickr_url', 'image_id', 'ids', 'captions', 'sentences'
])

def tokenize_captions(examples, is_train=True):
    captions = examples['text']
    inputs = tokenizer(
        captions, max_length=tokenizer.model_max_length, padding="max_length", truncation=True, return_tensors="pt"
    )
    return inputs.input_ids


train_transforms = transforms.Compose([
    transforms.Resize((resolution,resolution)),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5]),
])



def preprocess_train(examples):
    images = [image.convert("RGB") for image in examples['image']]
    examples["pixel_values"] = [train_transforms(image) for image in images]
    examples["input_ids"] = tokenize_captions(examples)
    return examples


train_dataset = dataset.with_transform(preprocess_train)

def collate_fn(examples):
    pixel_values = torch.stack([example["pixel_values"] for example in examples])
    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
    input_ids = torch.stack([example["input_ids"] for example in examples])
    return {"pixel_values": pixel_values, "input_ids": input_ids}

train_dataloader = torch.utils.data.DataLoader(
    train_dataset,
    shuffle=True,
    collate_fn=collate_fn,
    batch_size=train_batch_size,
    num_workers=0,
)

device = 'cuda'
weight_dtype = torch.bfloat16

text_encoder.to(device, dtype=weight_dtype)
vae.to(device, dtype=weight_dtype)
unet.to(device, dtype=weight_dtype)

num_train_epochs = math.ceil(max_train_steps * train_batch_size / len(train_dataset))
print("***** Running training *****")
print(f"  Num examples = {len(train_dataset)}")
print(f"  Num Epochs = {num_train_epochs}")
print(f"  Instantaneous batch size per device = {train_batch_size}")
print(f"  Total optimization steps = {max_train_steps}")

global_step = 0
initial_global_step = 0

progress_bar = tqdm(
    range(0, max_train_steps),
    initial=initial_global_step,
    desc="Steps",
)

losses = []
for epoch in range(num_train_epochs):
    for step, batch in enumerate(train_dataloader):
        latents = vae.encode(batch["pixel_values"].to(weight_dtype).to(device)).latent_dist.sample()
        latents = latents * vae.config.scaling_factor

        noise = torch.randn_like(latents)
        batch_size = latents.shape[0]
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=latents.device)
        timesteps = timesteps.long()

        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

        encoder_hidden_states = text_encoder(batch["input_ids"].to('cuda'), return_dict=False)[0]

        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]
        loss = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")

        loss.backward()
        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()

        if global_step % 100 == 0:
          current_lr = lr_scheduler.get_last_lr()[0]
          print(f"Step {global_step} | LR = {current_lr}")

        losses.append(loss.item())
        progress_bar.update(1)
        global_step += 1
        progress_bar.set_postfix(average_loss=np.mean(losses[-20:]), step=global_step)
        if global_step >= max_train_steps:
            break

import torch
import os
import matplotlib.pyplot as plt

save_dir = "saved_models"
os.makedirs(save_dir, exist_ok=True)

torch.save(unet.state_dict(), os.path.join(save_dir, "unet_final.pt"))
torch.save(vae.state_dict(), os.path.join(save_dir, "vae_final.pt"))
print("✅ Saved raw UNet and VAE weights.")

plt.figure(figsize=(10, 5))
plt.plot(losses, label="Training Loss")
plt.xlabel("Step")
plt.ylabel("Loss")
plt.title("UNet Training Loss Curve")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(save_dir, "training_loss_curve.png"))
plt.close()
torch.save(losses, os.path.join(save_dir, "loss_values.pt"))
print("✅ Saved loss plot and values.")

pipeline_save_path = os.path.join(save_dir, "full_pipeline")
pipe.save_pretrained(pipeline_save_path)
print(f"✅ Full pipeline saved to: {pipeline_save_path}")

prompt = "Beautiful giraffe running on sunset"
image = pipe(prompt, width=256, height=256).images[0]
image.resize((512, 512))
image

prompt = "Beautiful zebra running on sunset"
image = pipe(prompt, width=256, height=256).images[0]
image.resize((512, 512))
image

prompt = "Beautiful elephant running on sunset"
image = pipe(prompt, width=256, height=256).images[0]
image.resize((512, 512))
image

new_pipeline = DiffusionPipeline.from_pretrained(
    base_model_name,
    vae=vae,
    unet=unet,
)
new_pipeline.push_to_hub("AustinJiang/giraffe-zebra-translation", token='')

from diffusers import DiffusionPipeline
import torch

pipe = DiffusionPipeline.from_pretrained(
    "AustinJiang/giraffe-zebra-translation"
)
pipe.to("cuda")

prompt = "a dog walk under sunshine"
image = pipe(prompt, width=256, height=256).images[0]
image